{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int, {})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "VALID_YEARS = {2017, 2018, 2019, 2020, 2021}\n",
    "\n",
    "# Dictionary to keep track of the number of images downloaded per year\n",
    "images_per_year = defaultdict(int)\n",
    "\n",
    "images_per_year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "from selenium import webdriver\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from urllib.parse import urlparse\n",
    "import aiohttp\n",
    "import asyncio\n",
    "import time\n",
    "import nest_asyncio\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import io\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Define the path to the CSV file with image URLs\n",
    "csv_file_path = r\"C:\\Users\\hmane\\Desktop\\CLEAN_CT\\FB_IG_RACE_LGBTQ\\fb_ig_race_lgbtq_01012016_06132024_sentiment_raceterm_lgbtqterm_with_gender.csv\"\n",
    "\n",
    "# Define the base path to the folder where you want to save the images\n",
    "base_image_folder_path = r\"C:\\Users\\hmane\\Desktop\\label_studio_images\\images_yr2\"\n",
    "\n",
    "MAX_THREADS = 10\n",
    "MAX_RETRIES = 3  # For example, retry 3 times\n",
    "BATCH_SIZE = 10  # We want 300 images per folder\n",
    "IMAGE_LIMIT_PER_YEAR = 30  # We want to create three folders per year\n",
    "VALID_YEARS = { 2017, 2018, 2019, 2020, 2021}\n",
    "\n",
    "# Dictionary to keep track of the number of images downloaded per year\n",
    "images_per_year = defaultdict(int)\n",
    "\n",
    "def navigate_to_url(driver, url, retries=MAX_RETRIES):\n",
    "    for i in range(retries):\n",
    "        try:\n",
    "            driver.get(url)\n",
    "            return True  # Successfully navigated to the URL\n",
    "        except Exception as e:\n",
    "            print(f\"Attempt {i + 1} failed. Retrying...\")\n",
    "            if i == retries - 1:  # If this was the last attempt\n",
    "                print(f\"Failed to navigate to {url} after {retries} attempts.\")\n",
    "                return False  # Navigation failed after max retries\n",
    "        time.sleep(5)  # Delay for 5 seconds before retrying\n",
    "\n",
    "def is_valid_url(url):\n",
    "    try:\n",
    "        result = urlparse(url)\n",
    "        return all([result.scheme, result.netloc])\n",
    "    except ValueError:\n",
    "        return False\n",
    "\n",
    "async def image_exists(url):\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        try:\n",
    "            async with session.head(url, timeout=10) as response:\n",
    "                return response.status == 200\n",
    "        except Exception as e:\n",
    "            print(f\"Error checking existence of {url}: {e}\")\n",
    "            return False\n",
    "\n",
    "async def download_image_async(row_number, url, file_name, year):\n",
    "    # Determine the batch subfolder based on the number of images downloaded so far\n",
    "    batch_number = images_per_year[year] // BATCH_SIZE + 1\n",
    "\n",
    "    # If batch_number exceeds 3, do not download more images for the year\n",
    "    if batch_number > 3:\n",
    "        return\n",
    "\n",
    "    # Create the folder for the year and batch if it doesn't exist\n",
    "    batch_folder_path = os.path.join(base_image_folder_path, str(year), f'batch_{batch_number}')\n",
    "    os.makedirs(batch_folder_path, exist_ok=True)\n",
    "\n",
    "    async with aiohttp.ClientSession(connector=aiohttp.TCPConnector(limit=MAX_THREADS)) as session:\n",
    "        try:\n",
    "            async with session.get(url, timeout=30) as response:\n",
    "                if response.status == 200:\n",
    "                    file_content = await response.read()\n",
    "                    if not file_content or len(file_content) < 100:  # Check for very small or empty files\n",
    "                        print(f\"Row {row_number}: Image content is invalid or too small.\")\n",
    "                        return\n",
    "                    \n",
    "                    # Validate image content using PIL\n",
    "                    try:\n",
    "                        Image.open(io.BytesIO(file_content)).verify()\n",
    "                    except Exception as e:\n",
    "                        print(f\"Row {row_number}: Image content is invalid: {e}\")\n",
    "                        return\n",
    "\n",
    "                    file_path = os.path.join(batch_folder_path, file_name)\n",
    "                    with open(file_path, 'wb') as f:\n",
    "                        f.write(file_content)\n",
    "                    images_per_year[year] += 1  # Increment the count for the year\n",
    "                else:\n",
    "                    print(f\"Row {row_number}: Unexpected response status: {response.status}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Row {row_number}: Error downloading image: {e}\")\n",
    "\n",
    "async def extract_image_urls():\n",
    "    with open(csv_file_path, 'r', encoding='utf-8') as csv_file:\n",
    "        csv_reader = csv.DictReader(csv_file)\n",
    "        chunk = []\n",
    "        for row_number, row in enumerate(csv_reader, 1):\n",
    "            if row_number < 23480:\n",
    "                continue  # Skip rows until 23480\n",
    "\n",
    "            post_created_date = row['Post Created Date']\n",
    "            post_year = pd.to_datetime(post_created_date).year\n",
    "\n",
    "            if post_year not in VALID_YEARS or images_per_year[post_year] >= IMAGE_LIMIT_PER_YEAR:\n",
    "                continue  # Skip years not in the valid list or if limit reached\n",
    "\n",
    "            if row['Type'] == 'Photo':\n",
    "                image_url = row['Link']\n",
    "                if not is_valid_url(image_url):\n",
    "                    print(f\"Row {row_number}: Invalid URL: {image_url}\")\n",
    "                    continue\n",
    "\n",
    "                if not navigate_to_url(driver, image_url):\n",
    "                    continue\n",
    "\n",
    "                post_url = row['URL']\n",
    "                post_id = post_url.split(\"/\")[-1]\n",
    "\n",
    "                if row_number % 1000 == 0:\n",
    "                    print(f\"Row {row_number}: {post_id}\")\n",
    "                print(f\"Attempting to access URL: {image_url}\")\n",
    "\n",
    "                try:\n",
    "                    # wait for the image element to be present\n",
    "                    wait = WebDriverWait(driver, 10)\n",
    "                    img_element = wait.until(EC.presence_of_element_located((By.XPATH, '//img')))\n",
    "                    img_url = img_element.get_attribute(\"src\")\n",
    "\n",
    "                    if await image_exists(img_url):\n",
    "                        file_name = post_id + \".png\"\n",
    "\n",
    "                        chunk.append((row_number, img_url, file_name, post_year))\n",
    "\n",
    "                        if len(chunk) == BATCH_SIZE:\n",
    "                            yield chunk\n",
    "                            chunk = []\n",
    "                    else:\n",
    "                        print(f\"Row {row_number}: Image does not exist: {img_url}\")\n",
    "\n",
    "                except NoSuchElementException:\n",
    "                    print(f\"Row {row_number}: No image found on page: {image_url}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Row {row_number}: Unknown error downloading image: {image_url}\\n{str(e)}\")\n",
    "\n",
    "        # Yield any remaining tasks in the chunk\n",
    "        if chunk:\n",
    "            yield chunk\n",
    "\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument('--headless')\n",
    "driver = webdriver.Chrome(service=Service(r\"C:\\Users\\hmane\\Downloads\\chromedriver-win32\\chromedriver-win32\\chromedriver.exe\"), options=options)\n",
    "\n",
    "async def main():\n",
    "    async for batch in extract_image_urls():\n",
    "        await asyncio.gather(*[download_image_async(row_number, img_url, file_name, year) for row_number, img_url, file_name, year in batch])\n",
    "\n",
    "try:\n",
    "    asyncio.run(main())\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "\n",
    "try:\n",
    "    driver.quit()\n",
    "except Exception as e:\n",
    "    print(f\"Error while quitting the driver: {e}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
